{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "from korr_corpusreader import KorrIOBCorpusReader\n",
    "from training import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'templates' from '../templates.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from importlib import reload\n",
    "import training\n",
    "reload(training)\n",
    "from training import Trainer\n",
    "import templates\n",
    "reload(templates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from templates import template1, template2, template3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# on the NLT vm\n",
    "#t = Trainer(\"../lib/config/korr_nlp.json\")\n",
    "# on my local mac\n",
    "t = Trainer(\"../lib/config/korr_mac.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we load the current model just to extract some information\n",
    "import pickle\n",
    "\n",
    "with open(\"../lib/models/korrespondez_model_stage5.pickle\", \"rb\") as f:\n",
    "    m = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-PERauthor',\n",
       " 'B-PERaddressee',\n",
       " 'B-PLACEfrom',\n",
       " 'B-DATEletter',\n",
       " 'I-DATEletter',\n",
       " 'B-PERmentioned',\n",
       " 'B-PLACEmentioned',\n",
       " 'B-OBJ',\n",
       " 'I-OBJ',\n",
       " 'B-LIT',\n",
       " 'B-DATEmentioned',\n",
       " 'I-DATEmentioned',\n",
       " 'I-LIT',\n",
       " 'I-PERmentioned',\n",
       " 'I-PERauthor',\n",
       " 'B-ORGmentioned',\n",
       " 'I-ORGmentioned',\n",
       " 'I-PLACEmentioned',\n",
       " 'I-PERaddressee',\n",
       " 'B-PLACEto',\n",
       " 'I-PLACEfrom',\n",
       " 'B-DATErecieved',\n",
       " 'I-DATErecieved',\n",
       " 'B-DATEanswered',\n",
       " 'I-DATEanswered',\n",
       " 'B-ORGaddressee',\n",
       " 'I-ORGaddressee']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = list(m.classes_)\n",
    "labels.remove('O')\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fmambrini/anaconda/lib/python3.6/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/Users/fmambrini/anaconda/lib/python3.6/site-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.grid_search import RandomizedSearchCV\n",
    "\n",
    "import scipy\n",
    "\n",
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import scorers\n",
    "from sklearn_crfsuite import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f1_scorer = make_scorer(metrics.flat_f1_score,\n",
    "                        average='weighted', labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"/Users/fmambrini/Desktop/korrespondez_model_stage3_features.pickle\", \"wb\") as out:\n",
    "    pickle.dump(t.X_train, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"/Users/fmambrini/Desktop/korrespondez_model_stage3_targets.pickle\", \"wb\") as out:\n",
    "    pickle.dump(t.y_train, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"/Users/fmambrini/Desktop/korrespondez_model_stage3_scorer.pickle\", \"wb\") as out:\n",
    "    pickle.dump(f1_scorer, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From Annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "newcols = [\"words\", \"pos\", \"lemma\", \"textlayer\", \"chunk\", \"entityid\"]\n",
    "corpus_3 = KorrIOBCorpusReader(\"../data/IOB_GOLD/\", r\"[1-3].*\\.iob\", columntypes=newcols)\n",
    "corpus = KorrIOBCorpusReader(\"../data/IOB_GOLD/\", r\".*\\.iob\", columntypes=newcols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8080"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus_3.sents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "291594"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus.words())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the model in `t` with the frist 3 volumes. We use vol. 4 as a testbed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filesto4 = [c for c in corpus.fileids() if c.split(\"_\")[0] in [\"1\", \"2\", \"3\", \"4\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now we extract the target labels from vol. 4 that we'll use for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_labels(stage):\n",
    "    c = KorrIOBCorpusReader(\"../data/IOB_GOLD/\", r\"{}_.*\\.iob\".format(stage), columntypes=newcols)\n",
    "    y_targets = []\n",
    "    for s in c.full_tagged_sents():\n",
    "        sent = [t[-1] for t in s]\n",
    "        y_targets.append(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_stage_train_test(corpus, stage):\n",
    "    to_include = list(range(1,int(stage)))\n",
    "    files = [c for c in corpus.fileids() if int(c.split(\"_\")[0]) in to_include]\n",
    "    training = corpus.full_tagged_sents(fileids=files)\n",
    "    test_files = [c for c in corpus.fileids() if c.split(\"_\")[0] == str(stage)]\n",
    "    test = corpus.full_tagged_sents(fileids=test_files)\n",
    "    return training,test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_to_stage(trainer, stage, templ=template1):\n",
    "    \"\"\"Produce an evaluation and a trainer object using the materials \n",
    "    up to a stage n. Use stage 1 to n-1 as training and stage n as testing.\n",
    "\n",
    "    :param corpus: the general corpus\n",
    "    :param stage: int or string.\n",
    "    :return: tuple eval, trainer object    \n",
    "    \"\"\"\n",
    "    train_sent, test_sent = prepare_stage_train_test(trainer._corpus, stage)\n",
    "    trainer.training = train_sent\n",
    "    trainer.test = test_sent\n",
    "    trainer.set_feats_labels(templ)\n",
    "    print('training length: {} sentences\\ntesting length: {} sentences'.format(len(trainer.X_train), len(trainer.X_test)))\n",
    "    print(\"fitting...\")\n",
    "    trainer.fit()\n",
    "    print(\"evaluating...\")\n",
    "    ev = trainer.evaluate()\n",
    "    return ev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stage befor the last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t_tostage5 = Trainer(\"../lib/config/korr_mac.json\")\n",
    "#len(t_tostage4.training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9951\n",
      "training length: 9951 sentences\n",
      "testing length: 2418 sentences\n",
      "fitting...\n",
      "evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fmambrini/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/fmambrini/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1115: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/Users/fmambrini/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/fmambrini/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1115: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "ev = evaluate_to_stage(t_tostage5, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.65382651905\n"
     ]
    }
   ],
   "source": [
    "print(ev[\"F1_general\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Last stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15938"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_tostage6 = Trainer(\"../lib/config/korr_mac.json\")\n",
    "len(t_tostage6.training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training length: 12369 sentences\n",
      "testing length: 3569 sentences\n",
      "fitting...\n",
      "evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fmambrini/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/fmambrini/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1115: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/Users/fmambrini/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/fmambrini/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1115: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9min 46s, sys: 29.1 s, total: 10min 15s\n",
      "Wall time: 11min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ev6 = evaluate_to_stage(t_tostage6, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  precision    recall  f1-score   support\n",
      "\n",
      "  B-DATEanswered      0.000     0.000     0.000         0\n",
      "  I-DATEanswered      0.000     0.000     0.000         0\n",
      "    B-DATEletter      0.891     0.296     0.444       358\n",
      "    I-DATEletter      0.660     0.621     0.640       103\n",
      " B-DATEmentioned      0.947     0.546     0.693       196\n",
      " I-DATEmentioned      0.868     0.562     0.683       176\n",
      "  B-DATErecieved      0.000     0.000     0.000         0\n",
      "  I-DATErecieved      0.000     0.000     0.000         0\n",
      "           B-LIT      0.000     0.000     0.000         0\n",
      "           I-LIT      0.000     0.000     0.000         0\n",
      "           B-OBJ      0.778     0.359     0.491       156\n",
      "           I-OBJ      0.444     0.246     0.317        65\n",
      "  B-ORGaddressee      0.000     0.000     0.000         0\n",
      "  I-ORGaddressee      0.000     0.000     0.000         0\n",
      "  B-ORGmentioned      0.890     0.694     0.780       222\n",
      "  I-ORGmentioned      0.314     0.200     0.244        55\n",
      "  B-PERaddressee      0.960     0.820     0.884       266\n",
      "  I-PERaddressee      0.000     0.000     0.000         0\n",
      "     B-PERauthor      0.943     0.636     0.759       365\n",
      "     I-PERauthor      0.167     0.065     0.093        31\n",
      "  B-PERmentioned      0.814     0.753     0.783      1338\n",
      "  I-PERmentioned      0.724     0.620     0.668       297\n",
      "     B-PLACEfrom      0.970     0.696     0.810        92\n",
      "     I-PLACEfrom      0.000     0.000     0.000        12\n",
      "B-PLACEmentioned      0.926     0.725     0.813       517\n",
      "I-PLACEmentioned      0.520     0.283     0.366        46\n",
      "       B-PLACEto      0.000     0.000     0.000         2\n",
      "\n",
      "     avg / total      0.836     0.630     0.707      4297\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(ev6[\"F1_class\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t = Trainer(\"../lib/config/korr_nlp.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train,test = t.split(test_perc=0.15)\n",
    "t.train = train\n",
    "t.test = test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep in mind that we save the validation set (15% of the total) in the `Trainer.test` attribute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 39.1 s, sys: 759 ms, total: 39.9 s\n",
      "Wall time: 39.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "t.set_feats_labels(template1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define fixed parameters and parameters to search\n",
    "params_space = {\n",
    "    'c1': scipy.stats.expon(scale=0.5),\n",
    "    'c2': scipy.stats.expon(scale=0.05),\n",
    "}\n",
    "\n",
    "\n",
    "# search\n",
    "rs = RandomizedSearchCV(t.crf, params_space,\n",
    "                        cv=3,\n",
    "                        verbose=1,\n",
    "                        n_jobs=6,\n",
    "                        n_iter=50,\n",
    "                        scoring=f1_scorer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "rs.fit(t.X_train, t.y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimize the feature templates "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to define a cross evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t_tostage5 = Trainer(\"../lib/config/korr_mac.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9951\n",
      "training length: 9951 sentences\n",
      "testing length: 2418 sentences\n",
      "fitting...\n",
      "evaluating...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fmambrini/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/fmambrini/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1115: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/Users/fmambrini/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/fmambrini/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1115: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8min 42s, sys: 28.8 s, total: 9min 11s\n",
      "Wall time: 10min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ev5_templ2 = evaluate_to_stage(t_tostage5, 5, template3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  precision    recall  f1-score   support\n",
      "\n",
      "  B-DATEanswered      1.000     0.500     0.667        82\n",
      "  I-DATEanswered      1.000     0.464     0.634        84\n",
      "    B-DATEletter      0.882     0.944     0.912       374\n",
      "    I-DATEletter      0.908     0.972     0.939       995\n",
      " B-DATEmentioned      0.825     0.546     0.657       207\n",
      " I-DATEmentioned      0.721     0.728     0.725       103\n",
      "  B-DATErecieved      1.000     0.167     0.286        96\n",
      "  I-DATErecieved      0.941     0.158     0.271       101\n",
      "           B-LIT      0.083     1.000     0.154         1\n",
      "           I-LIT      0.091     1.000     0.167         1\n",
      "           B-OBJ      0.588     0.284     0.383       211\n",
      "           I-OBJ      0.343     0.230     0.275       100\n",
      "  B-ORGaddressee      0.000     0.000     0.000         0\n",
      "  I-ORGaddressee      0.000     0.000     0.000         0\n",
      "  B-ORGmentioned      0.915     0.639     0.753       269\n",
      "  I-ORGmentioned      0.833     0.426     0.563        47\n",
      "  B-PERaddressee      0.928     0.744     0.826       344\n",
      "  I-PERaddressee      0.000     0.000     0.000        63\n",
      "     B-PERauthor      0.422     0.173     0.245       359\n",
      "     I-PERauthor      0.000     0.000     0.000        10\n",
      "  B-PERmentioned      0.649     0.764     0.702      1473\n",
      "  I-PERmentioned      0.422     0.665     0.516       155\n",
      "     B-PLACEfrom      0.953     0.943     0.948       368\n",
      "     I-PLACEfrom      0.000     0.000     0.000         0\n",
      "B-PLACEmentioned      0.886     0.754     0.814       463\n",
      "I-PLACEmentioned      0.321     0.290     0.305        31\n",
      "       B-PLACEto      0.000     0.000     0.000         1\n",
      "\n",
      "     avg / total      0.765     0.699     0.709      5938\n",
      "\n"
     ]
    }
   ],
   "source": [
    " print(ev5_templ2[\"F1_class\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import ShuffleSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_learning_curve(estimator, title, X, y, scorer, ylim=None, cv=None,\n",
    "                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    \"\"\"\n",
    "    Generate a simple plot of the test and training learning curve.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    estimator : object type that implements the \"fit\" and \"predict\" methods\n",
    "        An object of that type which is cloned for each validation.\n",
    "\n",
    "    title : string\n",
    "        Title for the chart.\n",
    "\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        Training vector, where n_samples is the number of samples and\n",
    "        n_features is the number of features.\n",
    "\n",
    "    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n",
    "        Target relative to X for classification or regression;\n",
    "        None for unsupervised learning.\n",
    "\n",
    "    ylim : tuple, shape (ymin, ymax), optional\n",
    "        Defines minimum and maximum yvalues plotted.\n",
    "\n",
    "    cv : int, cross-validation generator or an iterable, optional\n",
    "        Determines the cross-validation splitting strategy.\n",
    "        Possible inputs for cv are:\n",
    "          - None, to use the default 3-fold cross-validation,\n",
    "          - integer, to specify the number of folds.\n",
    "          - An object to be used as a cross-validation generator.\n",
    "          - An iterable yielding train/test splits.\n",
    "\n",
    "        For integer/None inputs, if ``y`` is binary or multiclass,\n",
    "        :class:`StratifiedKFold` used. If the estimator is not a classifier\n",
    "        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n",
    "\n",
    "        Refer :ref:`User Guide <cross_validation>` for the various\n",
    "        cross-validators that can be used here.\n",
    "\n",
    "    n_jobs : integer, optional\n",
    "        Number of jobs to run in parallel (default 1).\n",
    "    \"\"\"\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, scoring=scorer, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "estimator = t.crf\n",
    "title = \"Learning curve (CRF)\"\n",
    "X = t.X_train\n",
    "y = t.y_train\n",
    "scorer = f1_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/share/.virtualenvs/nlppy3/lib/python3.4/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/share/.virtualenvs/nlppy3/lib/python3.4/site-packages/sklearn/metrics/classification.py:1115: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-55-fe6f2699b199>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'time'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'plot_learning_curve(estimator, title, X, y, scorer, cv = 4, n_jobs=3)\\nplt.show()'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/usr/local/share/.virtualenvs/nlppy3/lib/python3.4/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[1;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[0;32m   2118\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2119\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2120\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2121\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[1;32m/usr/local/share/.virtualenvs/nlppy3/lib/python3.4/site-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(f, *a, **k)\u001b[0m\n\u001b[0;32m    191\u001b[0m     \u001b[1;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 193\u001b[1;33m         \u001b[0mcall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    194\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/share/.virtualenvs/nlppy3/lib/python3.4/site-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[0;32m   1175\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1176\u001b[0m             \u001b[0mst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1177\u001b[1;33m             \u001b[0mexec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1178\u001b[0m             \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1179\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<ipython-input-51-9bddce588a95>\u001b[0m in \u001b[0;36mplot_learning_curve\u001b[1;34m(estimator, title, X, y, scorer, ylim, cv, n_jobs, train_sizes)\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Score\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m     train_sizes, train_scores, test_scores = learning_curve(\n\u001b[1;32m---> 50\u001b[1;33m         estimator, X, y, cv=cv, scoring=scorer, n_jobs=n_jobs, train_sizes=train_sizes)\n\u001b[0m\u001b[0;32m     51\u001b[0m     \u001b[0mtrain_scores_mean\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_scores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[0mtrain_scores_std\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_scores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/share/.virtualenvs/nlppy3/lib/python3.4/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36mlearning_curve\u001b[1;34m(estimator, X, y, groups, train_sizes, cv, scoring, exploit_incremental_learning, n_jobs, pre_dispatch, verbose)\u001b[0m\n\u001b[0;32m    770\u001b[0m             \u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscorer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mn_train_samples\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    771\u001b[0m             verbose, parameters=None, fit_params=None, return_train_score=True)\n\u001b[1;32m--> 772\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcv_iter\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    773\u001b[0m             for n_train_samples in train_sizes_abs)\n\u001b[0;32m    774\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/share/.virtualenvs/nlppy3/lib/python3.4/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    766\u001b[0m                 \u001b[1;31m# consumption.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    767\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 768\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    769\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    770\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/share/.virtualenvs/nlppy3/lib/python3.4/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    717\u001b[0m                     \u001b[0mensure_ready\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_managed_backend\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    718\u001b[0m                     \u001b[0mbackend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabort_everything\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mensure_ready\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mensure_ready\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 719\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    720\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/share/.virtualenvs/nlppy3/lib/python3.4/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    680\u001b[0m                 \u001b[1;31m# check if timeout supported in backend future implementation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    681\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;34m'timeout'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgetfullargspec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 682\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    683\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    684\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python3.4/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    591\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    592\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 593\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    594\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mready\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    595\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python3.4/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    588\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    589\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 590\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    591\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    592\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python3.4/threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    551\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 553\u001b[1;33m                 \u001b[0msignaled\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    554\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    555\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib/python3.4/threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    288\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    289\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 290\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    291\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    292\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEZCAYAAACNebLAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f4c6414e748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "plot_learning_curve(estimator, title, X, y, scorer, cv = 4, n_jobs=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper-parameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 50 candidates, totalling 150 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/fmambrini/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/fmambrini/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/fmambrini/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/fmambrini/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1115: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/Users/fmambrini/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/fmambrini/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1115: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/Users/fmambrini/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/fmambrini/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1115: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/Users/fmambrini/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/fmambrini/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/fmambrini/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1115: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/Users/fmambrini/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/fmambrini/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1115: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/Users/fmambrini/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/fmambrini/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/fmambrini/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1115: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/Users/fmambrini/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/fmambrini/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1115: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/Users/fmambrini/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/fmambrini/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1115: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/Users/fmambrini/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/fmambrini/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/fmambrini/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1115: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/Users/fmambrini/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/fmambrini/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/fmambrini/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1115: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/Users/fmambrini/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/fmambrini/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1115: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/Users/fmambrini/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/fmambrini/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/fmambrini/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1115: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/Users/fmambrini/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/fmambrini/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1115: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/Users/fmambrini/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/fmambrini/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/fmambrini/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1115: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/Users/fmambrini/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/fmambrini/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1115: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/Users/fmambrini/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/fmambrini/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1115: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/Users/fmambrini/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/fmambrini/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/fmambrini/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/fmambrini/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1115: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/Users/fmambrini/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/fmambrini/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1115: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/Users/fmambrini/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/fmambrini/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1115: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/Users/fmambrini/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/fmambrini/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/fmambrini/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1115: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/Users/fmambrini/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/fmambrini/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1115: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/Users/fmambrini/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/fmambrini/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1115: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/Users/fmambrini/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/fmambrini/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/fmambrini/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/fmambrini/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1115: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/Users/fmambrini/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/fmambrini/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1115: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/Users/fmambrini/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/fmambrini/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/fmambrini/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1115: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/Users/fmambrini/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/fmambrini/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1115: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/Users/fmambrini/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/fmambrini/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1115: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/Users/fmambrini/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/fmambrini/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1115: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed: 76.9min\n",
      "/Users/fmambrini/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/fmambrini/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1115: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/Users/fmambrini/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/fmambrini/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/fmambrini/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1115: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/Users/fmambrini/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/fmambrini/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/fmambrini/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1115: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n",
      "/Users/fmambrini/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/fmambrini/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1113: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/Users/fmambrini/anaconda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1115: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# define fixed parameters and parameters to search\n",
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True\n",
    ")\n",
    "params_space = {\n",
    "    'c1': scipy.stats.expon(scale=0.5),\n",
    "    'c2': scipy.stats.expon(scale=0.05),\n",
    "}\n",
    "\n",
    "# use the same metric for evaluation\n",
    "\n",
    "# search\n",
    "rs = RandomizedSearchCV(crf, params_space,\n",
    "                        cv=3,\n",
    "                        verbose=1,\n",
    "                        n_jobs=-1,\n",
    "                        n_iter=50,\n",
    "                        scoring=f1_scorer)\n",
    "rs.fit(t.X_train, t.y_train)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "nav_menu": {},
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
